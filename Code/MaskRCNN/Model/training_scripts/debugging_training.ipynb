{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0363901-d2ad-4f39-a262-2adeec5539d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.2 True\n"
     ]
    }
   ],
   "source": [
    "# the python script to train the masses with 'BENIGN & MALIGNANT' classess. Here we are treating the \n",
    "# soft_tissue, skin_thickening, and nipple_retraction as a same category - soft_tissue.\n",
    "\n",
    "# importing the required libraries\n",
    "\n",
    "# Check Pytorch installation\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "\n",
    "\n",
    "#importing packiage to be later use\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path as osp\n",
    "from pathlib import Path\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "766ff47d-f244-4a07-b5be-db196fdc8137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c44288ab-82b3-49e0-9013-53d17ee88fc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_random_seed(seed, deterministic=False):\n",
    "    \"\"\"Set random seed.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Seed to be used.\n",
    "        deterministic (bool): Whether to set the deterministic option for\n",
    "            CUDNN backend, i.e., set `torch.backends.cudnn.deterministic`\n",
    "            to True and `torch.backends.cudnn.benchmark` to False.\n",
    "            Default: False.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b55580b1-f158-478b-b7c5-ddb1c01e8eee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_dataset(cfg, default_args=None):\n",
    "    from .dataset_wrappers import (ConcatDataset, RepeatDataset,\n",
    "                                   ClassBalancedDataset, MultiImageMixDataset)\n",
    "    if isinstance(cfg, (list, tuple)):\n",
    "        dataset = ConcatDataset([build_dataset(c, default_args) for c in cfg])\n",
    "    elif cfg['type'] == 'ConcatDataset':\n",
    "        dataset = ConcatDataset(\n",
    "            [build_dataset(c, default_args) for c in cfg['datasets']],\n",
    "            cfg.get('separate_eval', True))\n",
    "    elif cfg['type'] == 'RepeatDataset':\n",
    "        dataset = RepeatDataset(\n",
    "            build_dataset(cfg['dataset'], default_args), cfg['times'])\n",
    "    elif cfg['type'] == 'ClassBalancedDataset':\n",
    "        dataset = ClassBalancedDataset(\n",
    "            build_dataset(cfg['dataset'], default_args), cfg['oversample_thr'])\n",
    "    elif cfg['type'] == 'MultiImageMixDataset':\n",
    "        cp_cfg = copy.deepcopy(cfg)\n",
    "        cp_cfg['dataset'] = build_dataset(cp_cfg['dataset'])\n",
    "        cp_cfg.pop('type')\n",
    "        dataset = MultiImageMixDataset(**cp_cfg)\n",
    "    elif isinstance(cfg.get('ann_file'), (list, tuple)):\n",
    "        dataset = _concat_dataset(cfg, default_args)\n",
    "    else:\n",
    "        dataset = build_from_cfg(cfg, DATASETS, default_args)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6d39f82-bd98-4a53-aa74-e4d2ffb830b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_detector(model,\n",
    "                   dataset,\n",
    "                   cfg,\n",
    "                   distributed=False,\n",
    "                   validate=False,\n",
    "                   timestamp=None,\n",
    "                   meta=None):\n",
    "    logger = get_root_logger(log_level=cfg.log_level)\n",
    "\n",
    "    # prepare data loaders\n",
    "    dataset = dataset if isinstance(dataset, (list, tuple)) else [dataset]\n",
    "    if 'imgs_per_gpu' in cfg.data:\n",
    "        logger.warning('\"imgs_per_gpu\" is deprecated in MMDet V2.0. '\n",
    "                       'Please use \"samples_per_gpu\" instead')\n",
    "        if 'samples_per_gpu' in cfg.data:\n",
    "            logger.warning(\n",
    "                f'Got \"imgs_per_gpu\"={cfg.data.imgs_per_gpu} and '\n",
    "                f'\"samples_per_gpu\"={cfg.data.samples_per_gpu}, \"imgs_per_gpu\"'\n",
    "                f'={cfg.data.imgs_per_gpu} is used in this experiments')\n",
    "        else:\n",
    "            logger.warning(\n",
    "                'Automatically set \"samples_per_gpu\"=\"imgs_per_gpu\"='\n",
    "                f'{cfg.data.imgs_per_gpu} in this experiments')\n",
    "        cfg.data.samples_per_gpu = cfg.data.imgs_per_gpu\n",
    "\n",
    "    runner_type = 'EpochBasedRunner' if 'runner' not in cfg else cfg.runner[\n",
    "        'type']\n",
    "    data_loaders = [\n",
    "        build_dataloader(\n",
    "            ds,\n",
    "            cfg.data.samples_per_gpu,\n",
    "            cfg.data.workers_per_gpu,\n",
    "            # `num_gpus` will be ignored if distributed\n",
    "            num_gpus=len(cfg.gpu_ids),\n",
    "            dist=distributed,\n",
    "            seed=cfg.seed,\n",
    "            runner_type=runner_type,\n",
    "            persistent_workers=cfg.data.get('persistent_workers', False))\n",
    "        for ds in dataset\n",
    "    ]\n",
    "\n",
    "    # put model on gpus\n",
    "    if distributed:\n",
    "        find_unused_parameters = cfg.get('find_unused_parameters', False)\n",
    "        # Sets the `find_unused_parameters` parameter in\n",
    "        # torch.nn.parallel.DistributedDataParallel\n",
    "        model = MMDistributedDataParallel(\n",
    "            model.cuda(),\n",
    "            device_ids=[torch.cuda.current_device()],\n",
    "            broadcast_buffers=False,\n",
    "            find_unused_parameters=find_unused_parameters)\n",
    "    else:\n",
    "        model = MMDataParallel(\n",
    "            model.cuda(cfg.gpu_ids[0]), device_ids=cfg.gpu_ids)\n",
    "\n",
    "    # build runner\n",
    "    optimizer = build_optimizer(model, cfg.optimizer)\n",
    "\n",
    "    if 'runner' not in cfg:\n",
    "        cfg.runner = {\n",
    "            'type': 'EpochBasedRunner',\n",
    "            'max_epochs': cfg.total_epochs\n",
    "        }\n",
    "        warnings.warn(\n",
    "            'config is now expected to have a `runner` section, '\n",
    "            'please set `runner` in your config.', UserWarning)\n",
    "    else:\n",
    "        if 'total_epochs' in cfg:\n",
    "            assert cfg.total_epochs == cfg.runner.max_epochs\n",
    "\n",
    "    runner = build_runner(\n",
    "        cfg.runner,\n",
    "        default_args=dict(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            work_dir=cfg.work_dir,\n",
    "            logger=logger,\n",
    "            meta=meta))\n",
    "\n",
    "    # an ugly workaround to make .log and .log.json filenames the same\n",
    "    runner.timestamp = timestamp\n",
    "\n",
    "    # fp16 setting\n",
    "    fp16_cfg = cfg.get('fp16', None)\n",
    "    if fp16_cfg is not None:\n",
    "        optimizer_config = Fp16OptimizerHook(\n",
    "            **cfg.optimizer_config, **fp16_cfg, distributed=distributed)\n",
    "    elif distributed and 'type' not in cfg.optimizer_config:\n",
    "        optimizer_config = OptimizerHook(**cfg.optimizer_config)\n",
    "    else:\n",
    "        optimizer_config = cfg.optimizer_config\n",
    "\n",
    "    # register hooks\n",
    "    runner.register_training_hooks(\n",
    "        cfg.lr_config,\n",
    "        optimizer_config,\n",
    "        cfg.checkpoint_config,\n",
    "        cfg.log_config,\n",
    "        cfg.get('momentum_config', None),\n",
    "        custom_hooks_config=cfg.get('custom_hooks', None))\n",
    "\n",
    "    if distributed:\n",
    "        if isinstance(runner, EpochBasedRunner):\n",
    "            runner.register_hook(DistSamplerSeedHook())\n",
    "\n",
    "    # register eval hooks\n",
    "    if validate:\n",
    "        # Support batch_size > 1 in validation\n",
    "        val_samples_per_gpu = cfg.data.val.pop('samples_per_gpu', 1)\n",
    "        if val_samples_per_gpu > 1:\n",
    "            # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "            cfg.data.val.pipeline = replace_ImageToTensor(\n",
    "                cfg.data.val.pipeline)\n",
    "        val_dataset = build_dataset(cfg.data.val, dict(test_mode=True))\n",
    "        val_dataloader = build_dataloader(\n",
    "            val_dataset,\n",
    "            samples_per_gpu=val_samples_per_gpu,\n",
    "            workers_per_gpu=cfg.data.workers_per_gpu,\n",
    "            dist=distributed,\n",
    "            shuffle=False)\n",
    "        eval_cfg = cfg.get('evaluation', {})\n",
    "        eval_cfg['by_epoch'] = cfg.runner['type'] != 'IterBasedRunner'\n",
    "        eval_hook = DistEvalHook if distributed else EvalHook\n",
    "        # In this PR (https://github.com/open-mmlab/mmcv/pull/1193), the\n",
    "        # priority of IterTimerHook has been modified from 'NORMAL' to 'LOW'.\n",
    "        runner.register_hook(\n",
    "            eval_hook(val_dataloader, **eval_cfg), priority='LOW')\n",
    "\n",
    "    if cfg.resume_from:\n",
    "        runner.resume(cfg.resume_from)\n",
    "    elif cfg.load_from:\n",
    "        runner.load_checkpoint(cfg.load_from)\n",
    "    runner.run(data_loaders, cfg.workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3eb092a-1a77-4c81-bfd1-88dcf6b35c8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    \"\"\"A facility for config and config files.\n",
    "\n",
    "    It supports common file formats as configs: python/json/yaml. The interface\n",
    "    is the same as a dict object and also allows access config values as\n",
    "    attributes.\n",
    "\n",
    "    Example:\n",
    "        >>> cfg = Config(dict(a=1, b=dict(b1=[0, 1])))\n",
    "        >>> cfg.a\n",
    "        1\n",
    "        >>> cfg.b\n",
    "        {'b1': [0, 1]}\n",
    "        >>> cfg.b.b1\n",
    "        [0, 1]\n",
    "        >>> cfg = Config.fromfile('tests/data/config/a.py')\n",
    "        >>> cfg.filename\n",
    "        \"/home/kchen/projects/mmcv/tests/data/config/a.py\"\n",
    "        >>> cfg.item4\n",
    "        'test'\n",
    "        >>> cfg\n",
    "        \"Config [path: /home/kchen/projects/mmcv/tests/data/config/a.py]: \"\n",
    "        \"{'item1': [1, 2], 'item2': {'a': 0}, 'item3': True, 'item4': 'test'}\"\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _validate_py_syntax(filename):\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            # Setting encoding explicitly to resolve coding issue on windows\n",
    "            content = f.read()\n",
    "        try:\n",
    "            ast.parse(content)\n",
    "        except SyntaxError as e:\n",
    "            raise SyntaxError('There are syntax errors in config '\n",
    "                              f'file {filename}: {e}')\n",
    "\n",
    "    @staticmethod\n",
    "    def _substitute_predefined_vars(filename, temp_config_name):\n",
    "        file_dirname = osp.dirname(filename)\n",
    "        file_basename = osp.basename(filename)\n",
    "        file_basename_no_extension = osp.splitext(file_basename)[0]\n",
    "        file_extname = osp.splitext(filename)[1]\n",
    "        support_templates = dict(\n",
    "            fileDirname=file_dirname,\n",
    "            fileBasename=file_basename,\n",
    "            fileBasenameNoExtension=file_basename_no_extension,\n",
    "            fileExtname=file_extname)\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            # Setting encoding explicitly to resolve coding issue on windows\n",
    "            config_file = f.read()\n",
    "        for key, value in support_templates.items():\n",
    "            regexp = r'\\{\\{\\s*' + str(key) + r'\\s*\\}\\}'\n",
    "            value = value.replace('\\\\', '/')\n",
    "            config_file = re.sub(regexp, value, config_file)\n",
    "        with open(temp_config_name, 'w', encoding='utf-8') as tmp_config_file:\n",
    "            tmp_config_file.write(config_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def _pre_substitute_base_vars(filename, temp_config_name):\n",
    "        \"\"\"Substitute base variable placehoders to string, so that parsing\n",
    "        would work.\"\"\"\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            # Setting encoding explicitly to resolve coding issue on windows\n",
    "            config_file = f.read()\n",
    "        base_var_dict = {}\n",
    "        regexp = r'\\{\\{\\s*' + BASE_KEY + r'\\.([\\w\\.]+)\\s*\\}\\}'\n",
    "        base_vars = set(re.findall(regexp, config_file))\n",
    "        for base_var in base_vars:\n",
    "            randstr = f'_{base_var}_{uuid.uuid4().hex.lower()[:6]}'\n",
    "            base_var_dict[randstr] = base_var\n",
    "            regexp = r'\\{\\{\\s*' + BASE_KEY + r'\\.' + base_var + r'\\s*\\}\\}'\n",
    "            config_file = re.sub(regexp, f'\"{randstr}\"', config_file)\n",
    "        with open(temp_config_name, 'w', encoding='utf-8') as tmp_config_file:\n",
    "            tmp_config_file.write(config_file)\n",
    "        return base_var_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def _substitute_base_vars(cfg, base_var_dict, base_cfg):\n",
    "        \"\"\"Substitute variable strings to their actual values.\"\"\"\n",
    "        cfg = copy.deepcopy(cfg)\n",
    "\n",
    "        if isinstance(cfg, dict):\n",
    "            for k, v in cfg.items():\n",
    "                if isinstance(v, str) and v in base_var_dict:\n",
    "                    new_v = base_cfg\n",
    "                    for new_k in base_var_dict[v].split('.'):\n",
    "                        new_v = new_v[new_k]\n",
    "                    cfg[k] = new_v\n",
    "                elif isinstance(v, (list, tuple, dict)):\n",
    "                    cfg[k] = Config._substitute_base_vars(\n",
    "                        v, base_var_dict, base_cfg)\n",
    "        elif isinstance(cfg, tuple):\n",
    "            cfg = tuple(\n",
    "                Config._substitute_base_vars(c, base_var_dict, base_cfg)\n",
    "                for c in cfg)\n",
    "        elif isinstance(cfg, list):\n",
    "            cfg = [\n",
    "                Config._substitute_base_vars(c, base_var_dict, base_cfg)\n",
    "                for c in cfg\n",
    "            ]\n",
    "        elif isinstance(cfg, str) and cfg in base_var_dict:\n",
    "            new_v = base_cfg\n",
    "            for new_k in base_var_dict[cfg].split('.'):\n",
    "                new_v = new_v[new_k]\n",
    "            cfg = new_v\n",
    "\n",
    "        return cfg\n",
    "\n",
    "    @staticmethod\n",
    "    def _file2dict(filename, use_predefined_variables=True):\n",
    "        filename = osp.abspath(osp.expanduser(filename))\n",
    "        check_file_exist(filename)\n",
    "        fileExtname = osp.splitext(filename)[1]\n",
    "        if fileExtname not in ['.py', '.json', '.yaml', '.yml']:\n",
    "            raise IOError('Only py/yml/yaml/json type are supported now!')\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as temp_config_dir:\n",
    "            temp_config_file = tempfile.NamedTemporaryFile(\n",
    "                dir=temp_config_dir, suffix=fileExtname)\n",
    "            if platform.system() == 'Windows':\n",
    "                temp_config_file.close()\n",
    "            temp_config_name = osp.basename(temp_config_file.name)\n",
    "            # Substitute predefined variables\n",
    "            if use_predefined_variables:\n",
    "                Config._substitute_predefined_vars(filename,\n",
    "                                                   temp_config_file.name)\n",
    "            else:\n",
    "                shutil.copyfile(filename, temp_config_file.name)\n",
    "            # Substitute base variables from placeholders to strings\n",
    "            base_var_dict = Config._pre_substitute_base_vars(\n",
    "                temp_config_file.name, temp_config_file.name)\n",
    "\n",
    "            if filename.endswith('.py'):\n",
    "                temp_module_name = osp.splitext(temp_config_name)[0]\n",
    "                sys.path.insert(0, temp_config_dir)\n",
    "                Config._validate_py_syntax(filename)\n",
    "                mod = import_module(temp_module_name)\n",
    "                sys.path.pop(0)\n",
    "                cfg_dict = {\n",
    "                    name: value\n",
    "                    for name, value in mod.__dict__.items()\n",
    "                    if not name.startswith('__')\n",
    "                }\n",
    "                # delete imported module\n",
    "                del sys.modules[temp_module_name]\n",
    "            elif filename.endswith(('.yml', '.yaml', '.json')):\n",
    "                import mmcv\n",
    "                cfg_dict = mmcv.load(temp_config_file.name)\n",
    "            # close temp file\n",
    "            temp_config_file.close()\n",
    "\n",
    "        # check deprecation information\n",
    "        if DEPRECATION_KEY in cfg_dict:\n",
    "            deprecation_info = cfg_dict.pop(DEPRECATION_KEY)\n",
    "            warning_msg = f'The config file {filename} will be deprecated ' \\\n",
    "                'in the future.'\n",
    "            if 'expected' in deprecation_info:\n",
    "                warning_msg += f' Please use {deprecation_info[\"expected\"]} ' \\\n",
    "                    'instead.'\n",
    "            if 'reference' in deprecation_info:\n",
    "                warning_msg += ' More information can be found at ' \\\n",
    "                    f'{deprecation_info[\"reference\"]}'\n",
    "            warnings.warn(warning_msg)\n",
    "\n",
    "        cfg_text = filename + '\\n'\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            # Setting encoding explicitly to resolve coding issue on windows\n",
    "            cfg_text += f.read()\n",
    "\n",
    "        if BASE_KEY in cfg_dict:\n",
    "            cfg_dir = osp.dirname(filename)\n",
    "            base_filename = cfg_dict.pop(BASE_KEY)\n",
    "            base_filename = base_filename if isinstance(\n",
    "                base_filename, list) else [base_filename]\n",
    "\n",
    "            cfg_dict_list = list()\n",
    "            cfg_text_list = list()\n",
    "            for f in base_filename:\n",
    "                _cfg_dict, _cfg_text = Config._file2dict(osp.join(cfg_dir, f))\n",
    "                cfg_dict_list.append(_cfg_dict)\n",
    "                cfg_text_list.append(_cfg_text)\n",
    "\n",
    "            base_cfg_dict = dict()\n",
    "            for c in cfg_dict_list:\n",
    "                duplicate_keys = base_cfg_dict.keys() & c.keys()\n",
    "                if len(duplicate_keys) > 0:\n",
    "                    raise KeyError('Duplicate key is not allowed among bases. '\n",
    "                                   f'Duplicate keys: {duplicate_keys}')\n",
    "                base_cfg_dict.update(c)\n",
    "\n",
    "            # Substitute base variables from strings to their actual values\n",
    "            cfg_dict = Config._substitute_base_vars(cfg_dict, base_var_dict,\n",
    "                                                    base_cfg_dict)\n",
    "\n",
    "            base_cfg_dict = Config._merge_a_into_b(cfg_dict, base_cfg_dict)\n",
    "            cfg_dict = base_cfg_dict\n",
    "\n",
    "            # merge cfg_text\n",
    "            cfg_text_list.append(cfg_text)\n",
    "            cfg_text = '\\n'.join(cfg_text_list)\n",
    "\n",
    "        return cfg_dict, cfg_text\n",
    "\n",
    "    @staticmethod\n",
    "    def _merge_a_into_b(a, b, allow_list_keys=False):\n",
    "        \"\"\"merge dict ``a`` into dict ``b`` (non-inplace).\n",
    "\n",
    "        Values in ``a`` will overwrite ``b``. ``b`` is copied first to avoid\n",
    "        in-place modifications.\n",
    "\n",
    "        Args:\n",
    "            a (dict): The source dict to be merged into ``b``.\n",
    "            b (dict): The origin dict to be fetch keys from ``a``.\n",
    "            allow_list_keys (bool): If True, int string keys (e.g. '0', '1')\n",
    "              are allowed in source ``a`` and will replace the element of the\n",
    "              corresponding index in b if b is a list. Default: False.\n",
    "\n",
    "        Returns:\n",
    "            dict: The modified dict of ``b`` using ``a``.\n",
    "\n",
    "        Examples:\n",
    "            # Normally merge a into b.\n",
    "            >>> Config._merge_a_into_b(\n",
    "            ...     dict(obj=dict(a=2)), dict(obj=dict(a=1)))\n",
    "            {'obj': {'a': 2}}\n",
    "\n",
    "            # Delete b first and merge a into b.\n",
    "            >>> Config._merge_a_into_b(\n",
    "            ...     dict(obj=dict(_delete_=True, a=2)), dict(obj=dict(a=1)))\n",
    "            {'obj': {'a': 2}}\n",
    "\n",
    "            # b is a list\n",
    "            >>> Config._merge_a_into_b(\n",
    "            ...     {'0': dict(a=2)}, [dict(a=1), dict(b=2)], True)\n",
    "            [{'a': 2}, {'b': 2}]\n",
    "        \"\"\"\n",
    "        b = b.copy()\n",
    "        for k, v in a.items():\n",
    "            if allow_list_keys and k.isdigit() and isinstance(b, list):\n",
    "                k = int(k)\n",
    "                if len(b) <= k:\n",
    "                    raise KeyError(f'Index {k} exceeds the length of list {b}')\n",
    "                b[k] = Config._merge_a_into_b(v, b[k], allow_list_keys)\n",
    "            elif isinstance(v,\n",
    "                            dict) and k in b and not v.pop(DELETE_KEY, False):\n",
    "                allowed_types = (dict, list) if allow_list_keys else dict\n",
    "                if not isinstance(b[k], allowed_types):\n",
    "                    raise TypeError(\n",
    "                        f'{k}={v} in child config cannot inherit from base '\n",
    "                        f'because {k} is a dict in the child config but is of '\n",
    "                        f'type {type(b[k])} in base config. You may set '\n",
    "                        f'`{DELETE_KEY}=True` to ignore the base config')\n",
    "                b[k] = Config._merge_a_into_b(v, b[k], allow_list_keys)\n",
    "            else:\n",
    "                b[k] = v\n",
    "        return b\n",
    "\n",
    "    @staticmethod\n",
    "    def fromfile(filename,\n",
    "                 use_predefined_variables=True,\n",
    "                 import_custom_modules=True):\n",
    "        cfg_dict, cfg_text = Config._file2dict(filename,\n",
    "                                               use_predefined_variables)\n",
    "        if import_custom_modules and cfg_dict.get('custom_imports', None):\n",
    "            import_modules_from_strings(**cfg_dict['custom_imports'])\n",
    "        return Config(cfg_dict, cfg_text=cfg_text, filename=filename)\n",
    "\n",
    "    @staticmethod\n",
    "    def fromstring(cfg_str, file_format):\n",
    "        \"\"\"Generate config from config str.\n",
    "\n",
    "        Args:\n",
    "            cfg_str (str): Config str.\n",
    "            file_format (str): Config file format corresponding to the\n",
    "               config str. Only py/yml/yaml/json type are supported now!\n",
    "\n",
    "        Returns:\n",
    "            obj:`Config`: Config obj.\n",
    "        \"\"\"\n",
    "        if file_format not in ['.py', '.json', '.yaml', '.yml']:\n",
    "            raise IOError('Only py/yml/yaml/json type are supported now!')\n",
    "        if file_format != '.py' and 'dict(' in cfg_str:\n",
    "            # check if users specify a wrong suffix for python\n",
    "            warnings.warn(\n",
    "                'Please check \"file_format\", the file format may be .py')\n",
    "        with tempfile.NamedTemporaryFile(\n",
    "                'w', encoding='utf-8', suffix=file_format,\n",
    "                delete=False) as temp_file:\n",
    "            temp_file.write(cfg_str)\n",
    "            # on windows, previous implementation cause error\n",
    "            # see PR 1077 for details\n",
    "        cfg = Config.fromfile(temp_file.name)\n",
    "        os.remove(temp_file.name)\n",
    "        return cfg\n",
    "\n",
    "    @staticmethod\n",
    "    def auto_argparser(description=None):\n",
    "        \"\"\"Generate argparser from config file automatically (experimental)\"\"\"\n",
    "        partial_parser = ArgumentParser(description=description)\n",
    "        partial_parser.add_argument('config', help='config file path')\n",
    "        cfg_file = partial_parser.parse_known_args()[0].config\n",
    "        cfg = Config.fromfile(cfg_file)\n",
    "        parser = ArgumentParser(description=description)\n",
    "        parser.add_argument('config', help='config file path')\n",
    "        add_args(parser, cfg)\n",
    "        return parser, cfg\n",
    "\n",
    "    def __init__(self, cfg_dict=None, cfg_text=None, filename=None):\n",
    "        if cfg_dict is None:\n",
    "            cfg_dict = dict()\n",
    "        elif not isinstance(cfg_dict, dict):\n",
    "            raise TypeError('cfg_dict must be a dict, but '\n",
    "                            f'got {type(cfg_dict)}')\n",
    "        for key in cfg_dict:\n",
    "            if key in RESERVED_KEYS:\n",
    "                raise KeyError(f'{key} is reserved for config file')\n",
    "\n",
    "        super(Config, self).__setattr__('_cfg_dict', ConfigDict(cfg_dict))\n",
    "        super(Config, self).__setattr__('_filename', filename)\n",
    "        if cfg_text:\n",
    "            text = cfg_text\n",
    "        elif filename:\n",
    "            with open(filename, 'r') as f:\n",
    "                text = f.read()\n",
    "        else:\n",
    "            text = ''\n",
    "        super(Config, self).__setattr__('_text', text)\n",
    "\n",
    "    @property\n",
    "    def filename(self):\n",
    "        return self._filename\n",
    "\n",
    "    @property\n",
    "    def text(self):\n",
    "        return self._text\n",
    "\n",
    "    @property\n",
    "    def pretty_text(self):\n",
    "\n",
    "        indent = 4\n",
    "\n",
    "        def _indent(s_, num_spaces):\n",
    "            s = s_.split('\\n')\n",
    "            if len(s) == 1:\n",
    "                return s_\n",
    "            first = s.pop(0)\n",
    "            s = [(num_spaces * ' ') + line for line in s]\n",
    "            s = '\\n'.join(s)\n",
    "            s = first + '\\n' + s\n",
    "            return s\n",
    "\n",
    "        def _format_basic_types(k, v, use_mapping=False):\n",
    "            if isinstance(v, str):\n",
    "                v_str = f\"'{v}'\"\n",
    "            else:\n",
    "                v_str = str(v)\n",
    "\n",
    "            if use_mapping:\n",
    "                k_str = f\"'{k}'\" if isinstance(k, str) else str(k)\n",
    "                attr_str = f'{k_str}: {v_str}'\n",
    "            else:\n",
    "                attr_str = f'{str(k)}={v_str}'\n",
    "            attr_str = _indent(attr_str, indent)\n",
    "\n",
    "            return attr_str\n",
    "\n",
    "        def _format_list(k, v, use_mapping=False):\n",
    "            # check if all items in the list are dict\n",
    "            if all(isinstance(_, dict) for _ in v):\n",
    "                v_str = '[\\n'\n",
    "                v_str += '\\n'.join(\n",
    "                    f'dict({_indent(_format_dict(v_), indent)}),'\n",
    "                    for v_ in v).rstrip(',')\n",
    "                if use_mapping:\n",
    "                    k_str = f\"'{k}'\" if isinstance(k, str) else str(k)\n",
    "                    attr_str = f'{k_str}: {v_str}'\n",
    "                else:\n",
    "                    attr_str = f'{str(k)}={v_str}'\n",
    "                attr_str = _indent(attr_str, indent) + ']'\n",
    "            else:\n",
    "                attr_str = _format_basic_types(k, v, use_mapping)\n",
    "            return attr_str\n",
    "\n",
    "        def _contain_invalid_identifier(dict_str):\n",
    "            contain_invalid_identifier = False\n",
    "            for key_name in dict_str:\n",
    "                contain_invalid_identifier |= \\\n",
    "                    (not str(key_name).isidentifier())\n",
    "            return contain_invalid_identifier\n",
    "\n",
    "        def _format_dict(input_dict, outest_level=False):\n",
    "            r = ''\n",
    "            s = []\n",
    "\n",
    "            use_mapping = _contain_invalid_identifier(input_dict)\n",
    "            if use_mapping:\n",
    "                r += '{'\n",
    "            for idx, (k, v) in enumerate(input_dict.items()):\n",
    "                is_last = idx >= len(input_dict) - 1\n",
    "                end = '' if outest_level or is_last else ','\n",
    "                if isinstance(v, dict):\n",
    "                    v_str = '\\n' + _format_dict(v)\n",
    "                    if use_mapping:\n",
    "                        k_str = f\"'{k}'\" if isinstance(k, str) else str(k)\n",
    "                        attr_str = f'{k_str}: dict({v_str}'\n",
    "                    else:\n",
    "                        attr_str = f'{str(k)}=dict({v_str}'\n",
    "                    attr_str = _indent(attr_str, indent) + ')' + end\n",
    "                elif isinstance(v, list):\n",
    "                    attr_str = _format_list(k, v, use_mapping) + end\n",
    "                else:\n",
    "                    attr_str = _format_basic_types(k, v, use_mapping) + end\n",
    "\n",
    "                s.append(attr_str)\n",
    "            r += '\\n'.join(s)\n",
    "            if use_mapping:\n",
    "                r += '}'\n",
    "            return r\n",
    "\n",
    "        cfg_dict = self._cfg_dict.to_dict()\n",
    "        text = _format_dict(cfg_dict, outest_level=True)\n",
    "        # copied from setup.cfg\n",
    "        yapf_style = dict(\n",
    "            based_on_style='pep8',\n",
    "            blank_line_before_nested_class_or_def=True,\n",
    "            split_before_expression_after_opening_paren=True)\n",
    "        text, _ = FormatCode(text, style_config=yapf_style, verify=True)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'Config (path: {self.filename}): {self._cfg_dict.__repr__()}'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._cfg_dict)\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return getattr(self._cfg_dict, name)\n",
    "\n",
    "    def __getitem__(self, name):\n",
    "        return self._cfg_dict.__getitem__(name)\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if isinstance(value, dict):\n",
    "            value = ConfigDict(value)\n",
    "        self._cfg_dict.__setattr__(name, value)\n",
    "\n",
    "    def __setitem__(self, name, value):\n",
    "        if isinstance(value, dict):\n",
    "            value = ConfigDict(value)\n",
    "        self._cfg_dict.__setitem__(name, value)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self._cfg_dict)\n",
    "\n",
    "    def __getstate__(self):\n",
    "        return (self._cfg_dict, self._filename, self._text)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        _cfg_dict, _filename, _text = state\n",
    "        super(Config, self).__setattr__('_cfg_dict', _cfg_dict)\n",
    "        super(Config, self).__setattr__('_filename', _filename)\n",
    "        super(Config, self).__setattr__('_text', _text)\n",
    "\n",
    "    def dump(self, file=None):\n",
    "        cfg_dict = super(Config, self).__getattribute__('_cfg_dict').to_dict()\n",
    "        if self.filename.endswith('.py'):\n",
    "            if file is None:\n",
    "                return self.pretty_text\n",
    "            else:\n",
    "                with open(file, 'w', encoding='utf-8') as f:\n",
    "                    f.write(self.pretty_text)\n",
    "        else:\n",
    "            import mmcv\n",
    "            if file is None:\n",
    "                file_format = self.filename.split('.')[-1]\n",
    "                return mmcv.dump(cfg_dict, file_format=file_format)\n",
    "            else:\n",
    "                mmcv.dump(cfg_dict, file)\n",
    "\n",
    "    def merge_from_dict(self, options, allow_list_keys=True):\n",
    "        \"\"\"Merge list into cfg_dict.\n",
    "\n",
    "        Merge the dict parsed by MultipleKVAction into this cfg.\n",
    "\n",
    "        Examples:\n",
    "            >>> options = {'model.backbone.depth': 50,\n",
    "            ...            'model.backbone.with_cp':True}\n",
    "            >>> cfg = Config(dict(model=dict(backbone=dict(type='ResNet'))))\n",
    "            >>> cfg.merge_from_dict(options)\n",
    "            >>> cfg_dict = super(Config, self).__getattribute__('_cfg_dict')\n",
    "            >>> assert cfg_dict == dict(\n",
    "            ...     model=dict(backbone=dict(depth=50, with_cp=True)))\n",
    "\n",
    "            # Merge list element\n",
    "            >>> cfg = Config(dict(pipeline=[\n",
    "            ...     dict(type='LoadImage'), dict(type='LoadAnnotations')]))\n",
    "            >>> options = dict(pipeline={'0': dict(type='SelfLoadImage')})\n",
    "            >>> cfg.merge_from_dict(options, allow_list_keys=True)\n",
    "            >>> cfg_dict = super(Config, self).__getattribute__('_cfg_dict')\n",
    "            >>> assert cfg_dict == dict(pipeline=[\n",
    "            ...     dict(type='SelfLoadImage'), dict(type='LoadAnnotations')])\n",
    "\n",
    "        Args:\n",
    "            options (dict): dict of configs to merge from.\n",
    "            allow_list_keys (bool): If True, int string keys (e.g. '0', '1')\n",
    "              are allowed in ``options`` and will replace the element of the\n",
    "              corresponding index in the config if the config is a list.\n",
    "              Default: True.\n",
    "        \"\"\"\n",
    "        option_cfg_dict = {}\n",
    "        for full_key, v in options.items():\n",
    "            d = option_cfg_dict\n",
    "            key_list = full_key.split('.')\n",
    "            for subkey in key_list[:-1]:\n",
    "                d.setdefault(subkey, ConfigDict())\n",
    "                d = d[subkey]\n",
    "            subkey = key_list[-1]\n",
    "            d[subkey] = v\n",
    "\n",
    "        cfg_dict = super(Config, self).__getattribute__('_cfg_dict')\n",
    "        super(Config, self).__setattr__(\n",
    "            '_cfg_dict',\n",
    "            Config._merge_a_into_b(\n",
    "                option_cfg_dict, cfg_dict, allow_list_keys=allow_list_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45caa9dc-ebfb-4eba-a245-1fdf29a77f19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define all the paths\n",
    "\n",
    "root_path = Path('/home/adil/Desktop/PhD_Data/PhD_Courses/First_Semester/Machine_Learning/Breast_Cancer_Detection/CBIS_DDSM/')\n",
    "train_json =  str (root_path / 'coco_files/annotations/train.json')\n",
    "test_json =  str(root_path / 'coco_files/annotations/validation.json')\n",
    "train_images = str(root_path / 'coco_files/train/')\n",
    "test_images =  str(root_path / 'coco_files/validation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41bfadbf-f30a-46d4-a177-4d7ff30b6b55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.exists(test_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "933fd798-ea18-4db1-9781-d8d029178753",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(test_json, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7715578c-d8e0-4e94-b355-3630c4be2cc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'license': 3,\n",
       " 'file_name': 'CBIS-DDSM_Calc-Test_P_01562_LEFT_MLO_08-29-2017-DDSM-NA-89693_1.000000-full mammogram images-75718_1-1.png',\n",
       " 'coco_url': '',\n",
       " 'height': 5544,\n",
       " 'width': 3736,\n",
       " 'date_captured': '2023-04-30',\n",
       " 'flickr_url': '',\n",
       " 'id': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['images'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05f421b4-1803-46dd-8050-d040140afd36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Supercategory': 'Breast_Cancer', 'id': 0, 'name': 'MASS_BENIGN'},\n",
       " {'Supercategory': 'Breast_Cancer', 'id': 1, 'name': 'MASS_MALIGNANT'},\n",
       " {'Supercategory': 'Breast_Cancer', 'id': 2, 'name': 'CALCIFICATION_BENIGN'},\n",
       " {'Supercategory': 'Breast_Cancer',\n",
       "  'id': 3,\n",
       "  'name': 'CALCIFICATION_MALIGNANT'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d49744a-4c02-4ae3-857b-bfd10238027c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'check_file_exist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(root_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBreast_detection_model/model/mask_rcnn/configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m cfg \u001b[38;5;241m=\u001b[39m \u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 262\u001b[0m, in \u001b[0;36mConfig.fromfile\u001b[0;34m(filename, use_predefined_variables, import_custom_modules)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfromfile\u001b[39m(filename,\n\u001b[1;32m    260\u001b[0m              use_predefined_variables\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    261\u001b[0m              import_custom_modules\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 262\u001b[0m     cfg_dict, cfg_text \u001b[38;5;241m=\u001b[39m \u001b[43mConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file2dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43muse_predefined_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m import_custom_modules \u001b[38;5;129;01mand\u001b[39;00m cfg_dict\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_imports\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    265\u001b[0m         import_modules_from_strings(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcfg_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustom_imports\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[6], line 112\u001b[0m, in \u001b[0;36mConfig._file2dict\u001b[0;34m(filename, use_predefined_variables)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_file2dict\u001b[39m(filename, use_predefined_variables\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    111\u001b[0m     filename \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mabspath(osp\u001b[38;5;241m.\u001b[39mexpanduser(filename))\n\u001b[0;32m--> 112\u001b[0m     \u001b[43mcheck_file_exist\u001b[49m(filename)\n\u001b[1;32m    113\u001b[0m     fileExtname \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39msplitext(filename)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fileExtname \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.yml\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'check_file_exist' is not defined"
     ]
    }
   ],
   "source": [
    "config_path = str(root_path / 'Breast_detection_model/model/mask_rcnn/configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py')\n",
    "cfg = Config.fromfile(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd489973-78dc-4a5f-97de-5323137630c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting the classes\n",
    "classes = ('BENIGN', 'MALIGNANT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4373a092-b1a9-4be6-ba9f-35d479983192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading dataset type\n",
    "cfg.dataset_type = 'COCODataset'\n",
    "\n",
    "# reading the data \n",
    "cfg.data.test.ann_file = test_json\n",
    "cfg.data.test.img_prefix = test_images\n",
    "cfg.data.test.classes = classes\n",
    "\n",
    "\n",
    "cfg.data.train.ann_file = train_json\n",
    "cfg.data.train.img_prefix = train_images\n",
    "cfg.data.train.classes = classes\n",
    "\n",
    "\n",
    "cfg.data.val.ann_file = test_json\n",
    "cfg.data.val.img_prefix = test_images\n",
    "cfg.data.val.classes = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47f57b77-db9b-48a8-9f94-4f5da9f709e4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "model = dict(\n",
      "    type='MaskRCNN',\n",
      "    backbone=dict(\n",
      "        type='ResNet',\n",
      "        depth=50,\n",
      "        num_stages=4,\n",
      "        out_indices=(0, 1, 2, 3),\n",
      "        frozen_stages=1,\n",
      "        norm_cfg=dict(type='BN', requires_grad=False),\n",
      "        norm_eval=True,\n",
      "        style='caffe',\n",
      "        init_cfg=dict(\n",
      "            type='Pretrained',\n",
      "            checkpoint='open-mmlab://detectron2/resnet50_caffe')),\n",
      "    neck=dict(\n",
      "        type='FPN',\n",
      "        in_channels=[256, 512, 1024, 2048],\n",
      "        out_channels=256,\n",
      "        num_outs=5),\n",
      "    rpn_head=dict(\n",
      "        type='RPNHead',\n",
      "        in_channels=256,\n",
      "        feat_channels=256,\n",
      "        anchor_generator=dict(\n",
      "            type='AnchorGenerator',\n",
      "            scales=[8],\n",
      "            ratios=[0.5, 1.0, 2.0],\n",
      "            strides=[4, 8, 16, 32, 64]),\n",
      "        bbox_coder=dict(\n",
      "            type='DeltaXYWHBBoxCoder',\n",
      "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
      "        loss_cls=dict(\n",
      "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
      "        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
      "    roi_head=dict(\n",
      "        type='StandardRoIHead',\n",
      "        bbox_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        bbox_head=dict(\n",
      "            type='Shared2FCBBoxHead',\n",
      "            in_channels=256,\n",
      "            fc_out_channels=1024,\n",
      "            roi_feat_size=7,\n",
      "            num_classes=2,\n",
      "            bbox_coder=dict(\n",
      "                type='DeltaXYWHBBoxCoder',\n",
      "                target_means=[0.0, 0.0, 0.0, 0.0],\n",
      "                target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
      "            reg_class_agnostic=False,\n",
      "            loss_cls=dict(\n",
      "                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n",
      "            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
      "        mask_roi_extractor=dict(\n",
      "            type='SingleRoIExtractor',\n",
      "            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),\n",
      "            out_channels=256,\n",
      "            featmap_strides=[4, 8, 16, 32]),\n",
      "        mask_head=dict(\n",
      "            type='FCNMaskHead',\n",
      "            num_convs=4,\n",
      "            in_channels=256,\n",
      "            conv_out_channels=256,\n",
      "            num_classes=2,\n",
      "            loss_mask=dict(\n",
      "                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))),\n",
      "    train_cfg=dict(\n",
      "        rpn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.7,\n",
      "                neg_iou_thr=0.3,\n",
      "                min_pos_iou=0.3,\n",
      "                match_low_quality=True,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=256,\n",
      "                pos_fraction=0.5,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=False),\n",
      "            allowed_border=-1,\n",
      "            pos_weight=-1,\n",
      "            debug=False),\n",
      "        rpn_proposal=dict(\n",
      "            nms_pre=2000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            assigner=dict(\n",
      "                type='MaxIoUAssigner',\n",
      "                pos_iou_thr=0.5,\n",
      "                neg_iou_thr=0.5,\n",
      "                min_pos_iou=0.5,\n",
      "                match_low_quality=True,\n",
      "                ignore_iof_thr=-1),\n",
      "            sampler=dict(\n",
      "                type='RandomSampler',\n",
      "                num=512,\n",
      "                pos_fraction=0.25,\n",
      "                neg_pos_ub=-1,\n",
      "                add_gt_as_proposals=True),\n",
      "            mask_size=28,\n",
      "            pos_weight=-1,\n",
      "            debug=False)),\n",
      "    test_cfg=dict(\n",
      "        rpn=dict(\n",
      "            nms_pre=1000,\n",
      "            max_per_img=1000,\n",
      "            nms=dict(type='nms', iou_threshold=0.7),\n",
      "            min_bbox_size=0),\n",
      "        rcnn=dict(\n",
      "            score_thr=0.05,\n",
      "            nms=dict(type='nms', iou_threshold=0.5),\n",
      "            max_per_img=100,\n",
      "            mask_thr_binary=0.5)))\n",
      "dataset_type = 'COCODataset'\n",
      "data_root = 'data/coco/'\n",
      "img_norm_cfg = dict(\n",
      "    mean=[20.70191168, 20.70191168, 20.70191168],\n",
      "    std=[1.0, 1.0, 1.0],\n",
      "    to_rgb=False)\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='LoadAnnotations',\n",
      "        with_bbox=True,\n",
      "        with_mask=True,\n",
      "        poly2mask=False),\n",
      "    dict(\n",
      "        type='Resize',\n",
      "        img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),\n",
      "                   (1333, 768), (1333, 800)],\n",
      "        multiscale_mode='value',\n",
      "        keep_ratio=True),\n",
      "    dict(type='RandomFlip', flip_ratio=0.5),\n",
      "    dict(\n",
      "        type='Normalize',\n",
      "        mean=[20.70191168, 20.70191168, 20.70191168],\n",
      "        std=[1.0, 1.0, 1.0],\n",
      "        to_rgb=False),\n",
      "    dict(type='Pad', size_divisor=32),\n",
      "    dict(type='DefaultFormatBundle'),\n",
      "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiScaleFlipAug',\n",
      "        img_scale=(1333, 800),\n",
      "        flip=False,\n",
      "        transforms=[\n",
      "            dict(type='Resize', keep_ratio=True),\n",
      "            dict(type='RandomFlip'),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[20.70191168, 20.70191168, 20.70191168],\n",
      "                std=[1.0, 1.0, 1.0],\n",
      "                to_rgb=False),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='ImageToTensor', keys=['img']),\n",
      "            dict(type='Collect', keys=['img'])\n",
      "        ])\n",
      "]\n",
      "data = dict(\n",
      "    samples_per_gpu=2,\n",
      "    workers_per_gpu=2,\n",
      "    train=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file=\n",
      "        '/home/adil/Desktop/PhD_Data/PhD_Courses/First_Semester/Machine_Learning/Breast_Cancer_Detection/CBIS_DDSM/coco_files/annotations/train.json',\n",
      "        img_prefix=\n",
      "        '/home/adil/Desktop/PhD_Data/PhD_Courses/First_Semester/Machine_Learning/Breast_Cancer_Detection/CBIS_DDSM/coco_files/train',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='LoadAnnotations',\n",
      "                with_bbox=True,\n",
      "                with_mask=True,\n",
      "                poly2mask=False),\n",
      "            dict(\n",
      "                type='Resize',\n",
      "                img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736),\n",
      "                           (1333, 768), (1333, 800)],\n",
      "                multiscale_mode='value',\n",
      "                keep_ratio=True),\n",
      "            dict(type='RandomFlip', flip_ratio=0.5),\n",
      "            dict(\n",
      "                type='Normalize',\n",
      "                mean=[20.70191168, 20.70191168, 20.70191168],\n",
      "                std=[1.0, 1.0, 1.0],\n",
      "                to_rgb=False),\n",
      "            dict(type='Pad', size_divisor=32),\n",
      "            dict(type='DefaultFormatBundle'),\n",
      "            dict(\n",
      "                type='Collect',\n",
      "                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
      "        ],\n",
      "        classes=('BENIGN', 'MALIGNANT')),\n",
      "    val=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file=\n",
      "        '/home/adil/Desktop/PhD_Data/PhD_Courses/First_Semester/Machine_Learning/Breast_Cancer_Detection/CBIS_DDSM/coco_files/annotations/validation.json',\n",
      "        img_prefix=\n",
      "        '/home/adil/Desktop/PhD_Data/PhD_Courses/First_Semester/Machine_Learning/Breast_Cancer_Detection/CBIS_DDSM/coco_files/validation',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[20.70191168, 20.70191168, 20.70191168],\n",
      "                        std=[1.0, 1.0, 1.0],\n",
      "                        to_rgb=False),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        classes=('BENIGN', 'MALIGNANT')),\n",
      "    test=dict(\n",
      "        type='CocoDataset',\n",
      "        ann_file=\n",
      "        '/home/adil/Desktop/PhD_Data/PhD_Courses/First_Semester/Machine_Learning/Breast_Cancer_Detection/CBIS_DDSM/coco_files/annotations/validation.json',\n",
      "        img_prefix=\n",
      "        '/home/adil/Desktop/PhD_Data/PhD_Courses/First_Semester/Machine_Learning/Breast_Cancer_Detection/CBIS_DDSM/coco_files/validation',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiScaleFlipAug',\n",
      "                img_scale=(1333, 800),\n",
      "                flip=False,\n",
      "                transforms=[\n",
      "                    dict(type='Resize', keep_ratio=True),\n",
      "                    dict(type='RandomFlip'),\n",
      "                    dict(\n",
      "                        type='Normalize',\n",
      "                        mean=[20.70191168, 20.70191168, 20.70191168],\n",
      "                        std=[1.0, 1.0, 1.0],\n",
      "                        to_rgb=False),\n",
      "                    dict(type='Pad', size_divisor=32),\n",
      "                    dict(type='ImageToTensor', keys=['img']),\n",
      "                    dict(type='Collect', keys=['img'])\n",
      "                ])\n",
      "        ],\n",
      "        classes=('BENIGN', 'MALIGNANT')))\n",
      "evaluation = dict(metric=['bbox', 'segm'], interval=1)\n",
      "optimizer = dict(type='SGD', lr=0.0025, momentum=0.9, weight_decay=0.0001)\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "lr_config = dict(\n",
      "    policy='step',\n",
      "    warmup=None,\n",
      "    warmup_iters=500,\n",
      "    warmup_ratio=0.001,\n",
      "    step=[8, 11])\n",
      "runner = dict(type='EpochBasedRunner', max_epochs=1)\n",
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(\n",
      "    interval=1,\n",
      "    hooks=[dict(type='TextLoggerHook'),\n",
      "           dict(type='TensorboardLoggerHook')])\n",
      "custom_hooks = [dict(type='NumClassCheckHook')]\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = '/home/adil/Desktop/PhD_Data/PhD_Courses/First_Semester/Machine_Learning/Breast_Cancer_Detection/CBIS_DDSM/Breast_detection_model/weights/coco_weights/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "opencv_num_threads = 0\n",
      "mp_start_method = 'fork'\n",
      "auto_scale_lr = dict(enable=False, base_batch_size=16)\n",
      "work_dir = './weights/'\n",
      "seed = 0\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Customizing the configuration\n",
    "\n",
    "# modify num classes of the model in box head and mask head\n",
    "cfg.model.roi_head.bbox_head.num_classes = len(classes)\n",
    "cfg.model.roi_head.mask_head.num_classes = len(classes)\n",
    "\n",
    "# setting number of samples per_gpu, number of workers and number of epochs\n",
    "# cfg.data.samples_per_gpu = 1\n",
    "# cfg.data.workers_per_gpu = 1\n",
    "cfg.runner.max_epochs = 1\n",
    "\n",
    "# Uploading the pretrained weights - here we will initialize network with CatchAll Algo weights to obtain a higher performance\n",
    "checkpoints_path = str(root_path / 'Breast_detection_model/weights/coco_weights/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth')\n",
    "cfg.load_from  = checkpoints_path\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "os.makedirs('./weights', exist_ok=True)\n",
    "cfg.work_dir = './weights/'\n",
    "\n",
    "# The original learning rate (LR) is set for 8-GPU training.\n",
    "# We divide it by 8 since we only use one GPU.\n",
    "cfg.optimizer.lr = 0.02 / 8\n",
    "cfg.lr_config.warmup = None\n",
    "cfg.log_config.interval = 1\n",
    "\n",
    "# We can set the evaluation interval to reduce the evaluation times\n",
    "cfg.evaluation.interval = 1\n",
    "\n",
    "# We can set the checkpoint saving interval to reduce the storage cost\n",
    "cfg.checkpoint_config.interval = 1\n",
    "\n",
    "# Set seed thus the results are more reproducible\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "# We can also use tensorboard to log the training process\n",
    "cfg.log_config.hooks = [\n",
    "    dict(type='TextLoggerHook'),\n",
    "    dict(type='TensorboardLoggerHook')]\n",
    "\n",
    "# We can initialize the logger for training and have a look\n",
    "# at the final config used for training\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d687ecb1-b758-4c5e-9ce3-eabac933ac66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.74s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([\n",
       "  CocoDataset Train dataset with number of images 2354, and instance counts: \n",
       "  +------------+-------+---------------+-------+----------+-------+----------+-------+----------+-------+\n",
       "  | category   | count | category      | count | category | count | category | count | category | count |\n",
       "  +------------+-------+---------------+-------+----------+-------+----------+-------+----------+-------+\n",
       "  |            |       |               |       |          |       |          |       |          |       |\n",
       "  | 0 [BENIGN] | 1630  | 1 [MALIGNANT] | 1120  |          |       |          |       |          |       |\n",
       "  +------------+-------+---------------+-------+----------+-------+----------+-------+----------+-------+],)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build dataset\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "datasets, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bd05c44-f1fa-4e5f-bbf1-a009fd95e771",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build the detector\n",
    "model = build_detector(cfg.model)\n",
    "model.CLASSES = classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eb2c9d4-cb43-4fe7-8380-079bb75a0c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 01:04:51,671 - mmdet - INFO - load checkpoint from local path: /home/adil/Desktop/PhD_Data/PhD_Courses/First_Semester/Machine_Learning/Breast_Cancer_Detection/CBIS_DDSM/Breast_detection_model/weights/coco_weights/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.16s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 01:04:51,778 - mmdet - WARNING - The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for roi_head.bbox_head.fc_cls.weight: copying a param with shape torch.Size([81, 1024]) from checkpoint, the shape in current model is torch.Size([3, 1024]).\n",
      "size mismatch for roi_head.bbox_head.fc_cls.bias: copying a param with shape torch.Size([81]) from checkpoint, the shape in current model is torch.Size([3]).\n",
      "size mismatch for roi_head.bbox_head.fc_reg.weight: copying a param with shape torch.Size([320, 1024]) from checkpoint, the shape in current model is torch.Size([8, 1024]).\n",
      "size mismatch for roi_head.bbox_head.fc_reg.bias: copying a param with shape torch.Size([320]) from checkpoint, the shape in current model is torch.Size([8]).\n",
      "size mismatch for roi_head.mask_head.conv_logits.weight: copying a param with shape torch.Size([80, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 256, 1, 1]).\n",
      "size mismatch for roi_head.mask_head.conv_logits.bias: copying a param with shape torch.Size([80]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "2023-04-30 01:04:51,784 - mmdet - INFO - Start running, host: adil@adil, work_dir: /home/adil/Desktop/PhD_Data/PhD_Courses/First_Semester/Machine_Learning/Breast_Cancer_Detection/CBIS_DDSM/Breast_detection_model/model/training_scripts/weights\n",
      "2023-04-30 01:04:51,785 - mmdet - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) NumClassCheckHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) IterTimerHook                      \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(LOW         ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) NumClassCheckHook                  \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      "(VERY_LOW    ) TensorboardLoggerHook              \n",
      " -------------------- \n",
      "2023-04-30 01:04:51,785 - mmdet - INFO - workflow: [('train', 1)], max: 1 epochs\n",
      "2023-04-30 01:04:51,786 - mmdet - INFO - Checkpoints will be saved to /home/adil/Desktop/PhD_Data/PhD_Courses/First_Semester/Machine_Learning/Breast_Cancer_Detection/CBIS_DDSM/Breast_detection_model/model/training_scripts/weights by HardDiskBackend.\n",
      "/home/adil/miniconda3/envs/dlearning/lib/python3.9/site-packages/numpy/core/getlimits.py:500: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/adil/miniconda3/envs/dlearning/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/home/adil/miniconda3/envs/dlearning/lib/python3.9/site-packages/numpy/core/getlimits.py:500: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/home/adil/miniconda3/envs/dlearning/lib/python3.9/site-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "2023-04-30 01:04:56,582 - mmdet - INFO - Epoch [1][1/1177]\tlr: 2.500e-03, eta: 1:18:22, time: 3.999, data_time: 2.529, memory: 3254, loss_rpn_cls: 0.1488, loss_rpn_bbox: 0.0094, loss_cls: 1.3895, acc: 14.3555, loss_bbox: 0.0200, loss_mask: 0.6739, loss: 2.2416\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 132.00 MiB (GPU 0; 3.95 GiB total capacity; 2.82 GiB already allocated; 116.75 MiB free; 3.18 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create work_dir and calling the train function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m mmcv\u001b[38;5;241m.\u001b[39mmkdir_or_exist(osp\u001b[38;5;241m.\u001b[39mabspath(cfg\u001b[38;5;241m.\u001b[39mwork_dir))\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistributed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dlearning/lib/python3.9/site-packages/mmdet/apis/train.py:203\u001b[0m, in \u001b[0;36mtrain_detector\u001b[0;34m(model, dataset, cfg, distributed, validate, timestamp, meta)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mload_from:\n\u001b[1;32m    202\u001b[0m     runner\u001b[38;5;241m.\u001b[39mload_checkpoint(cfg\u001b[38;5;241m.\u001b[39mload_from)\n\u001b[0;32m--> 203\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dlearning/lib/python3.9/site-packages/mmcv/runner/epoch_based_runner.py:127\u001b[0m, in \u001b[0;36mEpochBasedRunner.run\u001b[0;34m(self, data_loaders, workflow, max_epochs, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_epochs:\n\u001b[1;32m    126\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m             \u001b[43mepoch_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# wait for some hooks like loggers to finish\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_run\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlearning/lib/python3.9/site-packages/mmcv/runner/epoch_based_runner.py:51\u001b[0m, in \u001b[0;36mEpochBasedRunner.train\u001b[0;34m(self, data_loader, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbefore_train_iter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_iter(data_batch, train_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mafter_train_iter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_train_epoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlearning/lib/python3.9/site-packages/mmcv/runner/base_runner.py:307\u001b[0m, in \u001b[0;36mBaseRunner.call_hook\u001b[0;34m(self, fn_name)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call all hooks.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03m    fn_name (str): The function name in each hook to be called, such as\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;03m        \"before_train_epoch\".\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hooks:\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dlearning/lib/python3.9/site-packages/mmcv/runner/hooks/optimizer.py:35\u001b[0m, in \u001b[0;36mOptimizerHook.after_train_iter\u001b[0;34m(self, runner)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mafter_train_iter\u001b[39m(\u001b[38;5;28mself\u001b[39m, runner):\n\u001b[1;32m     34\u001b[0m     runner\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 35\u001b[0m     \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_clip \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m         grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_grads(runner\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mparameters())\n",
      "File \u001b[0;32m~/miniconda3/envs/dlearning/lib/python3.9/site-packages/torch/tensor.py:245\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    238\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    239\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    243\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    244\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 245\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dlearning/lib/python3.9/site-packages/torch/autograd/__init__.py:145\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m--> 145\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dlearning/lib/python3.9/site-packages/torch/autograd/function.py:89\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dlearning/lib/python3.9/site-packages/torch/autograd/function.py:210\u001b[0m, in \u001b[0;36monce_differentiable.<locals>.wrapper\u001b[0;34m(ctx, *args)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(ctx, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 210\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_grad_enabled():\n\u001b[1;32m    213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/dlearning/lib/python3.9/site-packages/mmcv/ops/roi_align.py:112\u001b[0m, in \u001b[0;36mRoIAlignFunction.backward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;129m@once_differentiable\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackward\u001b[39m(ctx, grad_output):\n\u001b[1;32m    111\u001b[0m     rois, argmax_y, argmax_x \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39msaved_tensors\n\u001b[0;32m--> 112\u001b[0m     grad_input \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# complex head architecture may cause grad_output uncontiguous.\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     grad_output \u001b[38;5;241m=\u001b[39m grad_output\u001b[38;5;241m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 132.00 MiB (GPU 0; 3.95 GiB total capacity; 2.82 GiB already allocated; 116.75 MiB free; 3.18 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "# Create work_dir and calling the train function\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "train_detector(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78da498-2c49-443a-9067-620957944ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

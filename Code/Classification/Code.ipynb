{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c49a360",
   "metadata": {},
   "source": [
    "# Breast Cancer Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3f8730",
   "metadata": {},
   "source": [
    "## Group 9 - Spring 22/23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6323191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependincies\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b8fedc",
   "metadata": {},
   "source": [
    "# Deep Learning Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "743ae888",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBIS_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class inhereted from PyTorch's Dataset\n",
    "    attr:\n",
    "    annotations_dir: dir of csv file with all metadata\n",
    "    root_dir: root_dir :)\n",
    "    abnoramility: {mass, calc}\n",
    "    mamo_orientation: {MLO, CC} MLO-> front view, CC-> side view\n",
    "    breast: {LEFT, RIGHT}\n",
    "    image_type: {cropped images, full mammogram images, ROI mask images}\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 annotations_dir, \n",
    "                 root_dir, \n",
    "                 abnormality='mass', \n",
    "                 mamo_orientation='all', \n",
    "                 breast='all', \n",
    "                 image_type='cropped images',\n",
    "                 RGB=False,\n",
    "                 blur_aug='weak',\n",
    "                 perspective_aug='weak'):\n",
    "        \n",
    "        self.abnormality = abnormality\n",
    "        self.mamo_orientation = mamo_orientation\n",
    "        self.breast = breast\n",
    "        self.image_type = image_type\n",
    "        df = pd.read_csv(annotations_dir)\n",
    "        self.annotations = self.subset_dataframe(df)\n",
    "        self.root_dir = root_dir\n",
    "        self.RGB = RGB\n",
    "        self.blur_aug= blur_aug.lower()\n",
    "        self.perspective_aug= perspective_aug.lower()\n",
    "        \n",
    "        IMG_WIDTH, IMG_HEIGHT = 256, 256\n",
    "        \n",
    "        transform = [transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.RandomHorizontalFlip(p=0),\n",
    "                    transforms.RandomRotation(degrees=(0,0))]\n",
    "                \n",
    "        self.trasformation = transforms.Compose(transform)\n",
    "        \n",
    "    def subset_dataframe(self, df: pd.DataFrame)->pd.DataFrame:\n",
    "        if (self.mamo_orientation=='all')&(self.breast=='all'):\n",
    "            return df[(df.image_type==self.image_type)&\n",
    "                    (df.abnormality==self.abnormality)].reset_index(drop=True)\n",
    "        elif (self.mamo_orientation=='all'):\n",
    "            return df[(df.image_type==self.image_type)&\n",
    "                      (df.Breast==self.breast)&\n",
    "                      (df.abnormality==self.abnormality)].reset_index(drop=True)\n",
    "        elif (self.breast=='all'):\n",
    "            return df[(df.image_type==self.image_type)&\n",
    "                      (df.mamo_orientation==self.mamo_orientation)&\n",
    "                      (df.abnormality==self.abnormality)].reset_index(drop=True)\n",
    "        else:\n",
    "            return df[(df.image_type==self.image_type)&\n",
    "                      (df.Breast==self.breast)&\n",
    "                      (df.abnormality==self.abnormality)&\n",
    "                      (df.mamo_orientation==self.mamo_orientation)].reset_index(drop=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = os.path.join(self.root_dir, self.annotations.iloc[index]['image_path'])\n",
    "        sample = Image.open(path)\n",
    "        \n",
    "        if self.RGB:\n",
    "            sample = self.trasformation(sample).repeat(3,1,1)\n",
    "        else:\n",
    "            sample = self.trasformation(sample)\n",
    "        target_class = torch.tensor([int(self.annotations.iloc[index]['malignant'])]).to(torch.float32)\n",
    "        \n",
    "        #random guassian blur and noise augmentation\n",
    "        #initialize with no effect\n",
    "        blur, std, p_perspective = 0,0,0 #guassian blur and perspective control variable (random variable to control when augmentation is done)\n",
    "        if self.blur_aug == 'weak':\n",
    "            blur = np.random.choice([0,7])\n",
    "            std = np.random.uniform(low=0, high=0.01)\n",
    "        elif self.blur_aug == 'strong':\n",
    "            blur = np.random.choice([7,15])\n",
    "            std = np.random.uniform(low=0, high=0.8)\n",
    "        elif self.blur_aug == 'none':\n",
    "            blur, std = 0, 0\n",
    "        else:\n",
    "            raise RuntimeError(f\"Augmentation {self.blur_aug} is not implemented! choose normal, strong, or none\")\n",
    "            \n",
    "        if self.perspective_aug == 'weak':\n",
    "            p_perspective = 0.3\n",
    "        elif self.perspective_aug == 'strong':\n",
    "            p_perspective = 0.7\n",
    "        elif self.perspective_aug == 'none':\n",
    "            p_perspective = 0\n",
    "        else:\n",
    "            raise RuntimeError(f\"Augmentation {self.perspective_aug} is not implemented! choose normal, strong, or none\")\n",
    "\n",
    "        if blur:\n",
    "            sample = transforms.Compose([transforms.GaussianBlur(kernel_size = blur)])(sample)\n",
    "\n",
    "        #sample = sample +  np.random.normal(0, std, sample.shape) #this was too much noise so we dropped the idea\n",
    "         \n",
    "        sample = transforms.Compose([transforms.RandomPerspective(distortion_scale=0.5, p=p_perspective)])(sample)\n",
    "        \n",
    "        return (sample, target_class)\n",
    "    \n",
    "    def show_image(self, index):\n",
    "        sample = self.__getitem__(index)[0]\n",
    "        PIL_transform = transforms.ToPILImage()\n",
    "        PIL_transform(sample).show()\n",
    "    \n",
    "    def train_test_split(self):\n",
    "        self.train_idx = self.annotations[self.annotations['TrainTest']=='Training'].index.to_list()\n",
    "        self.test_idx = self.annotations[self.annotations['TrainTest']=='Test'].index.to_list()\n",
    "        \n",
    "        return Subset(self, self.train_idx), Subset(self, self.test_idx)\n",
    "    \n",
    "    def train_val_test_split(self, val_ratio=0.2):\n",
    "        self.train_idx = self.annotations[self.annotations['TrainTest']=='Training'].index.to_list()\n",
    "        self.test_idx = self.annotations[self.annotations['TrainTest']=='Test'].index.to_list()\n",
    "        \n",
    "        self.val_idx = random.sample(self.train_idx, round(len(self.train_idx)*val_ratio))\n",
    "        self.train_idx = list(set(self.train_idx) - set(self.val_idx))\n",
    "        \n",
    "        return Subset(self, self.train_idx), Subset(self, self.val_idx), Subset(self, self.test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e82b737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseline_model(nn.Module): #alexnet\n",
    "    \"\"\"\n",
    "    Baseline model which is a variant of AlexNet\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 11, stride=4, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 192, 5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(192)\n",
    "        self.pool2 = nn.MaxPool2d(3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, 3, stride=1, padding=1)\n",
    "        #self.conv4 = nn.Conv2d(384, 256, 3, stride=1, padding=1)\n",
    "        #self.conv5 = nn.Conv2d(256, 256, 3, stride=1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(3, 2)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(384*7*7, 1024)#(256*7*7, 1024)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(1024, 128)\n",
    "        self.fc3 = nn.Linear(128, 1)        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        #x = F.relu(self.conv4(x))\n",
    "        #x = self.pool3(F.relu(self.conv5(x)))\n",
    "        x = self.pool3(x)\n",
    "        x = self.drop(x)\n",
    "        x = x.view(-1, 384*7*7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b6b4aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "csv_dir = 'Dataset/csv/annotations.csv'\n",
    "root_dir = 'Dataset/'\n",
    "\n",
    "Data = CBIS_Dataset(annotations_dir=csv_dir, root_dir=root_dir, RGB=True, image_type='full mammogram images', blur_aug='none', perspective_aug='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619138d5",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9168c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \n",
      "Training Loss: 0.7295196075593272 \tValidation Loss: 0.703574612736702\n",
      "Training F1: 0.35966679290592785 \tValidation F1: 0.0           \n",
      "Training Recall: 0.48185518097757185, \tValidation Recall: 0.0           \n",
      "Training Precision: 0.35892621950771353, \tValidation Precision: 0.0 \n",
      " ******************************\n",
      "Validation Loss Decreased(inf--->5.628597) \t Saving The Model\n",
      "Epoch 2 \n",
      "Training Loss: 0.6918938390670284 \tValidation Loss: 0.6705066785216331\n",
      "Training F1: 0.375526453143109 \tValidation F1: 0.6402393121143121           \n",
      "Training Recall: 0.380322665394572, \tValidation Recall: 0.7249935919343814           \n",
      "Training Precision: 0.4795690934593982, \tValidation Precision: 0.5888435990338163 \n",
      " ******************************\n",
      "Validation Loss Decreased(5.628597--->5.364053) \t Saving The Model\n",
      "Epoch 3 \n",
      "Training Loss: 0.6964101791381836 \tValidation Loss: 0.6933934316039085\n",
      "Training F1: 0.4329604627327457 \tValidation F1: 0.16260822510822512           \n",
      "Training Recall: 0.5618589614719645, \tValidation Recall: 0.10016377080521817           \n",
      "Training Precision: 0.47211999249011705, \tValidation Precision: 0.5744047619047619 \n",
      " ******************************\n",
      "Epoch 4 \n",
      "Training Loss: 0.6830092880033678 \tValidation Loss: 0.6717530712485313\n",
      "Training F1: 0.3907537917468653 \tValidation F1: 0.6523523773574252           \n",
      "Training Recall: 0.409864693796832, \tValidation Recall: 0.725960794236723           \n",
      "Training Precision: 0.4975334544095155, \tValidation Precision: 0.6016963364293086 \n",
      " ******************************\n",
      "Epoch 5 \n",
      "Training Loss: 0.6665179748688975 \tValidation Loss: 0.6751491650938988\n",
      "Training F1: 0.5907504038705362 \tValidation F1: 0.6305775707815556           \n",
      "Training Recall: 0.6472188812995764, \tValidation Recall: 0.6693295739348372           \n",
      "Training Precision: 0.564014066373499, \tValidation Precision: 0.6103524743230626 \n",
      " ******************************\n",
      "Epoch 6 \n",
      "Training Loss: 0.669279198492727 \tValidation Loss: 0.6831642761826515\n",
      "Training F1: 0.5897870620523972 \tValidation F1: 0.5850483479159949           \n",
      "Training Recall: 0.6673511297063182, \tValidation Recall: 0.6214633196395967           \n",
      "Training Precision: 0.5562662066249903, \tValidation Precision: 0.5637823245240126 \n",
      " ******************************\n",
      "Epoch 7 \n",
      "Training Loss: 0.6675652130957572 \tValidation Loss: 0.701528936624527\n",
      "Training F1: 0.5740403491067244 \tValidation F1: 0.4777740395675178           \n",
      "Training Recall: 0.6345863686844324, \tValidation Recall: 0.3873818277310925           \n",
      "Training Precision: 0.5819051314030665, \tValidation Precision: 0.640530303030303 \n",
      " ******************************\n",
      "Epoch 8 \n",
      "Training Loss: 0.6613419248211768 \tValidation Loss: 0.6830721274018288\n",
      "Training F1: 0.5120527217937696 \tValidation F1: 0.6121978569055182           \n",
      "Training Recall: 0.5677661178884585, \tValidation Recall: 0.6410979962160304           \n",
      "Training Precision: 0.5388998584795136, \tValidation Precision: 0.5993627706572754 \n",
      " ******************************\n",
      "Epoch 9 \n",
      "Training Loss: 0.6754038679984308 \tValidation Loss: 0.6785153821110725\n",
      "Training F1: 0.5283771620627187 \tValidation F1: 0.5558958953106045           \n",
      "Training Recall: 0.6255656223610383, \tValidation Recall: 0.5347695707070708           \n",
      "Training Precision: 0.5408581461647303, \tValidation Precision: 0.58589070243482 \n",
      " ******************************\n",
      "Epoch 10 \n",
      "Training Loss: 0.6527888582598779 \tValidation Loss: 0.7161532416939735\n",
      "Training F1: 0.5779205251935392 \tValidation F1: 0.24876615380451694           \n",
      "Training Recall: 0.6118896899232462, \tValidation Recall: 0.15972089271237877           \n",
      "Training Precision: 0.6157213562801516, \tValidation Precision: 0.6072916666666667 \n",
      " ******************************\n",
      "Epoch 11 \n",
      "Training Loss: 0.6495393764588141 \tValidation Loss: 0.7032481804490089\n",
      "Training F1: 0.5605501305347821 \tValidation F1: 0.6045487740044193           \n",
      "Training Recall: 0.6088006528064888, \tValidation Recall: 0.6019312220502235           \n",
      "Training Precision: 0.5947393454987931, \tValidation Precision: 0.627522167160325 \n",
      " ******************************\n",
      "Epoch 12 \n",
      "Training Loss: 0.6591692720690081 \tValidation Loss: 0.7077190577983856\n",
      "Training F1: 0.569495596900777 \tValidation F1: 0.3923144609014174           \n",
      "Training Recall: 0.602187808529564, \tValidation Recall: 0.2815430119416188           \n",
      "Training Precision: 0.6015611513495525, \tValidation Precision: 0.6696428571428572 \n",
      " ******************************\n",
      "Epoch 13 \n",
      "Training Loss: 0.6597550934360873 \tValidation Loss: 0.6699587404727936\n",
      "Training F1: 0.447449794780828 \tValidation F1: 0.6796651987683923           \n",
      "Training Recall: 0.5026166654899303, \tValidation Recall: 0.7723870798319327           \n",
      "Training Precision: 0.5307591204620069, \tValidation Precision: 0.6137545532046811 \n",
      " ******************************\n",
      "Validation Loss Decreased(5.364053--->5.359670) \t Saving The Model\n",
      "Epoch 14 \n",
      "Training Loss: 0.6292500188273769 \tValidation Loss: 0.6897337883710861\n",
      "Training F1: 0.6250738574912805 \tValidation F1: 0.4267687392862708           \n",
      "Training Recall: 0.7055949819070262, \tValidation Recall: 0.32626296132488086           \n",
      "Training Precision: 0.5986177855040239, \tValidation Precision: 0.6450892857142857 \n",
      " ******************************\n",
      "Epoch 15 \n",
      "Training Loss: 0.6516492251426943 \tValidation Loss: 0.674330823123455\n",
      "Training F1: 0.5469136081360766 \tValidation F1: 0.6290651024382432           \n",
      "Training Recall: 0.6113690216094592, \tValidation Recall: 0.6556360263403608           \n",
      "Training Precision: 0.6097547354624119, \tValidation Precision: 0.6114069152381847 \n",
      " ******************************\n",
      "Epoch 16 \n",
      "Training Loss: 0.6332444875471054 \tValidation Loss: 0.7300809025764465\n",
      "Training F1: 0.6225769148820194 \tValidation F1: 0.5132010753004437           \n",
      "Training Recall: 0.6554913654983565, \tValidation Recall: 0.45119047619047625           \n",
      "Training Precision: 0.6277425128219096, \tValidation Precision: 0.6335227272727273 \n",
      " ******************************\n",
      "Epoch 17 \n",
      "Training Loss: 0.6284651179467479 \tValidation Loss: 0.7031655088067055\n",
      "Training F1: 0.5773045630056388 \tValidation F1: 0.5419337307152875           \n",
      "Training Recall: 0.5738232806231908, \tValidation Recall: 0.4786612139795189           \n",
      "Training Precision: 0.6384126283794216, \tValidation Precision: 0.6436627302436126 \n",
      " ******************************\n",
      "Epoch 18 \n",
      "Training Loss: 0.6312280066551701 \tValidation Loss: 0.6670791059732437\n",
      "Training F1: 0.606696106355875 \tValidation F1: 0.6570827270517673           \n",
      "Training Recall: 0.6404941972316883, \tValidation Recall: 0.7128454835498179           \n",
      "Training Precision: 0.6138868148293937, \tValidation Precision: 0.6194744126487546 \n",
      " ******************************\n",
      "Validation Loss Decreased(5.359670--->5.336633) \t Saving The Model\n",
      "Epoch 19 \n",
      "Training Loss: 0.624298757122409 \tValidation Loss: 0.6848828494548798\n",
      "Training F1: 0.5902330373468077 \tValidation F1: 0.6148312550826782           \n",
      "Training Recall: 0.6253400273332621, \tValidation Recall: 0.6609052916038209           \n",
      "Training Precision: 0.6221062466650811, \tValidation Precision: 0.5801909729202686 \n",
      " ******************************\n",
      "Epoch 20 \n",
      "Training Loss: 0.6090303534461606 \tValidation Loss: 0.6672776564955711\n",
      "Training F1: 0.6272121950270603 \tValidation F1: 0.6667985851867431           \n",
      "Training Recall: 0.6333576228525295, \tValidation Recall: 0.7346682014880545           \n",
      "Training Precision: 0.6516122173043175, \tValidation Precision: 0.6211731981468824 \n",
      " ******************************\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCH = 20\n",
    "LEARNING_RATE = 3e-4 #if you know you know \n",
    "\n",
    "train, val, test = Data.train_val_test_split()\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "#mod = baseline_model().to(device)\n",
    "#optimizer = Adam(mod.parameters(), lr=LEARNING_RATE)\n",
    "#loss_f = nn.BCELoss()\n",
    "\n",
    "mod = models.alexnet(pretrained=True).to(device)\n",
    "\n",
    "for param in mod.features.parameters(): param.requires_grad=False\n",
    "mod.classifier[-1] = nn.Linear(4096, 1)\n",
    "mod = mod.to(device)\n",
    "\n",
    "optimizer = Adam(mod.classifier.parameters(), lr=LEARNING_RATE)\n",
    "loss_f = nn.BCEWithLogitsLoss()\n",
    "min_valid_loss = np.inf\n",
    "\n",
    "train_results = {'loss':[], 'f1':[], 'precision':[], 'recall': []}\n",
    "val_results = {'loss':[], 'f1':[], 'precision':[], 'recall': []}\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    train_loss, f1_train, recall_train, precision_train = 0.0, 0.0, 0.0, 0.0\n",
    "    for batch in train_loader:\n",
    "        x,y = batch\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        pred = mod(x)\n",
    "        loss = loss_f(pred, y)\n",
    "        \n",
    "        f1_train_temp = f1_score(y.to('cpu').detach(), torch.sigmoid(pred).round().to('cpu').detach())\n",
    "        recall_train_temp = recall_score(y.to('cpu').detach(), torch.sigmoid(pred).round().to('cpu').detach(), zero_division=0)\n",
    "        precision_train_temp = precision_score(y.to('cpu').detach(), torch.sigmoid(pred).round().to('cpu').detach(), zero_division=0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del x\n",
    "        del y\n",
    "        if device == 'cuda': torch.cuda.empty_cache()\n",
    "        train_loss += loss.item()\n",
    "        f1_train += f1_train_temp\n",
    "        recall_train += recall_train_temp\n",
    "        precision_train += precision_train_temp\n",
    "            \n",
    "    val_loss, f1_val, recall_val, precision_val = 0.0, 0.0, 0.0, 0.0\n",
    "    mod.eval()    \n",
    "    for val_batch in val_loader:\n",
    "        x,y = val_batch\n",
    "        x,y = x.to(device), y.to(device)\n",
    "    \n",
    "        pred_val = mod(x)\n",
    "        loss = loss_f(pred_val, y)\n",
    "        \n",
    "        f1_val_temp = f1_score(y.to('cpu').detach(), torch.sigmoid(pred_val).round().round().to('cpu').detach())\n",
    "        recall_val_temp = recall_score(y.to('cpu').detach(), torch.sigmoid(pred_val).round().round().to('cpu').detach(), zero_division=0)\n",
    "        precision_val_temp = precision_score(y.to('cpu').detach(), torch.sigmoid(pred_val).round().round().to('cpu').detach() , zero_division=0)\n",
    "        \n",
    "        del x\n",
    "        del y\n",
    "        if device == 'cuda': torch.cuda.empty_cache()\n",
    "        val_loss += loss.item()\n",
    "        f1_val += f1_val_temp\n",
    "        recall_val += recall_val_temp\n",
    "        precision_val += precision_val_temp\n",
    "        \n",
    "    print(f'Epoch {epoch+1} \\nTraining Loss: {train_loss / len(train_loader)} \\tValidation Loss: {val_loss / len(val_loader)}')\n",
    "    print(f'Training F1: {f1_train/len(train_loader)} \\tValidation F1: {f1_val/ len(val_loader)} \\\n",
    "          \\nTraining Recall: {recall_train/ len(train_loader)}, \\tValidation Recall: {recall_val/len(val_loader)} \\\n",
    "          \\nTraining Precision: {precision_train/ len(train_loader)}, \\tValidation Precision: {precision_val/len(val_loader)}','\\n', 30*'*')\n",
    "    \n",
    "    train_results['loss'].append(train_loss/len(train_loader))\n",
    "    train_results['f1'].append(f1_train/len(train_loader))\n",
    "    train_results['precision'].append(recall_train/len(train_loader))\n",
    "    train_results['recall'].append(precision_train/len(train_loader))\n",
    "    \n",
    "    val_results['loss'].append(val_loss/len(val_loader))\n",
    "    val_results['f1'].append(f1_val/len(val_loader))\n",
    "    val_results['precision'].append(recall_val/len(val_loader))\n",
    "    val_results['recall'].append(precision_val/len(val_loader))\n",
    "     \n",
    "    if min_valid_loss > val_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{val_loss:.6f}) \\t Saving The Model')\n",
    "        min_valid_loss = val_loss\n",
    "         \n",
    "        # Saving State Dict\n",
    "        #torch.save(mod.state_dict(), 'saved_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c777dce",
   "metadata": {},
   "source": [
    "# Validation set Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65a57fb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EXP_NAME = 'aug_horizontal'\n",
    "i=0\n",
    "for metric in train_results.keys():\n",
    "    plt.plot(range(len(train_results[metric])), train_results[metric])\n",
    "    plt.plot(range(len(val_results[metric])), val_results[metric])\n",
    "    plt.title(metric)\n",
    "    plt.legend(['train', 'validation'])\n",
    "    #plt.show()\n",
    "    plt.savefig(f'{EXP_NAME}{i}.jpeg')\n",
    "    plt.close()\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769aefbc",
   "metadata": {},
   "source": [
    "# Retrain with Validation set to test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e410054",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCH = 30\n",
    "LEARNING_RATE = 3e-4 #if you know you know \n",
    "\n",
    "train, test = Data.train_test_split()\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "mod = models.alexnet(pretrained=True).to(device)\n",
    "\n",
    "for param in mod.features.parameters(): param.requires_grad=False\n",
    "mod.classifier[-1] = nn.Linear(4096, 1)\n",
    "mod = mod.to(device)\n",
    "\n",
    "optimizer = Adam(mod.classifier.parameters(), lr=LEARNING_RATE)\n",
    "loss_f = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_results = {'loss':[], 'f1':[], 'precision':[], 'recall': []}\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for batch in train_loader:\n",
    "        x,y = batch\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        pred = mod(x)\n",
    "        loss = loss_f(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        del x\n",
    "        del y\n",
    "        \n",
    "        torch.save(mod.state_dict(), f'{EXP_NAME}saved_model.pth')\n",
    "        \n",
    "        if device == 'cuda': torch.cuda.empty_cache()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2524cc3",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc8206e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "y_test = []\n",
    "test_loader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "mod.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x,y = batch\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        y_test_pred = torch.sigmoid(mod(x))\n",
    "        y_pred_tag = torch.round(y_test_pred)\n",
    "        y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "        y_test.append(np.array(y.to('cpu')))\n",
    "\n",
    "#y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a3e2855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aug_horizontal \n",
      "confusion matrix:\n",
      " [[ 63  16]\n",
      " [155 129]] \n",
      "f1:0.6013986013986014\n",
      "recall:0.8896551724137931\n",
      "precision:0.45422535211267606\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "y_test = np.vstack(y_test)\n",
    "y_pred = np.vstack(y_pred_list)\n",
    "\n",
    "f1_test = f1_train_temp = f1_score(y_test, y_pred)\n",
    "recal_test = recall_train_temp = recall_score(y_test, y_pred, zero_division=0)\n",
    "precision_test = precision_train_temp = precision_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "print(EXP_NAME, '\\nconfusion matrix:\\n',confusion_matrix(y_pred, y_test), f'\\nf1:{f1_test}\\nrecall:{recal_test}\\nprecision:{precision_test}') \n",
    "\n",
    "results_dict = {'confusion matrix': str(confusion_matrix(y_pred, y_test)),\n",
    "               'f1': str(f1_test),\n",
    "               'recall': str(recal_test),\n",
    "               'precision': str(precision_test)}\n",
    "\n",
    "with open(f'{EXP_NAME}results_dict.txt', 'w') as convert_file:\n",
    "     convert_file.write(json.dumps(results_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f050157",
   "metadata": {},
   "source": [
    "# SVM + PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91e6928",
   "metadata": {},
   "source": [
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c52798",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Returns transformed principal components\n",
    "def train_test_transformed_PCA(train, test, components):\n",
    "    # Train\n",
    "    pca_train = PCA(n_components = components)\n",
    "    pca_train.fit(train)\n",
    "    transformed_data_train = pca_train.transform(train)\n",
    "    print(sum(pca_train.explained_variance_ratio_))\n",
    "\n",
    "\n",
    "    # Test\n",
    "    pca_test = PCA(n_components = components)\n",
    "    pca_test.fit(test)\n",
    "    transformed_data_test = pca_test.transform(test)\n",
    "    print(sum(pca_test.explained_variance_ratio_))\n",
    "    \n",
    "    return transformed_data_train, transformed_data_test\n",
    "\n",
    "# function that you will use to convert matrix to dataframe, useful for visulization. \n",
    "def conf_matrix_to_df(conf_matrix, target_names):\n",
    "    return pd.DataFrame(conf_matrix, columns=target_names, index=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dfca6f",
   "metadata": {},
   "source": [
    "## Pipeline to create 3 subsets of full alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d7d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make each model independent:\n",
    "\n",
    "# 1. Full model\n",
    "mod_full = models.alexnet(pretrained=True).to(device)\n",
    "for param in mod_full.features.parameters(): param.requires_grad=False\n",
    "mod_full.classifier[-1] = nn.Linear(4096, 1)\n",
    "mod_full = mod_full.to(device)\n",
    "mod_full.load_state_dict(torch.load('./results/horizontal_flipping/aug_horizontalsaved_model.pth', \n",
    "                    map_location=torch.device('cpu')))\n",
    "mod_full.classifier = mod_full.classifier[:5:]\n",
    "print(mod_full.classifier)\n",
    "\n",
    "# 2. Truncated model (-last layer)\n",
    "mod_trunc1 = models.alexnet(pretrained=True).to(device)\n",
    "for param in mod_trunc1.features.parameters(): param.requires_grad=False\n",
    "mod_trunc1.classifier[-1] = nn.Linear(4096, 1)\n",
    "mod_trunc1.load_state_dict(torch.load('./results/horizontal_flipping/aug_horizontalsaved_model.pth', \n",
    "                    map_location=torch.device('cpu')))\n",
    "mod_trunc1.classifier = mod_trunc1.classifier[:3:]\n",
    "mod_trunc1 = mod_trunc1.to(device)\n",
    "print(mod_trunc1.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7364a323",
   "metadata": {},
   "source": [
    "## Fill separate model train/test subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d7fe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCH = 1\n",
    "# Collect Train/Test\n",
    "train, test = Data.train_test_split()\n",
    "train_loader = DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "########### Train ###########\n",
    "\n",
    "# Features\n",
    "dat_x_full_train = []\n",
    "dat_x_trunc1_train = []\n",
    "# Targets\n",
    "dat_y_full_train = []\n",
    "dat_y_trunc1_train = []\n",
    "\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for batch in train_loader:\n",
    "        x,y = batch \n",
    "        x,y = x.to(device), y.to(device)\n",
    "        # 1. Full model\n",
    "        pred = mod_full(x)\n",
    "        pred = np.array(pred.to('cpu').detach())\n",
    "        y = np.array(y.to('cpu'))\n",
    "        dat_x_full_train.append(pred)\n",
    "        dat_y_full_train.append(y)\n",
    "        \n",
    "        # 2. Truncated model\n",
    "        pred = mod_trunc1(x)\n",
    "        pred = np.array(pred.to('cpu').detach())\n",
    "        dat_x_trunc1_train.append(pred)\n",
    "        dat_y_trunc1_train.append(y)\n",
    "        \n",
    "########### Test ###########\n",
    "\n",
    "# Features\n",
    "dat_x_full_test = []\n",
    "dat_x_trunc1_test = []\n",
    "# Targets\n",
    "dat_y_full_test = []\n",
    "dat_y_trunc1_test = []\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    for batch in test_loader:\n",
    "        x,y = batch #(,)\n",
    "        x,y = x.to(device), y.to(device)\n",
    "        # 1. Full model\n",
    "        pred = mod_full(x)\n",
    "        pred = np.array(pred.to('cpu').detach())\n",
    "        y = np.array(y.to('cpu'))\n",
    "        dat_x_full_test.append(pred)\n",
    "        dat_y_full_test.append(y)\n",
    "        \n",
    "        # 2. Truncated model\n",
    "        pred = mod_trunc1(x)\n",
    "        pred = np.array(pred.to('cpu').detach())\n",
    "        dat_x_trunc1_test.append(pred)\n",
    "        dat_y_trunc1_test.append(y)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2006f3b",
   "metadata": {},
   "source": [
    "### Reshape train/test to numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a8761",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## Train\n",
    "# Features\n",
    "x_full_train = np.array(dat_x_full_train).reshape(len(dat_x_full_train),  np.array(dat_x_full_train).shape[2])\n",
    "x_trunc1_train = np.array(dat_x_trunc1_train).reshape(len(dat_x_trunc1_train), np.array(dat_x_trunc1_train).shape[2])\n",
    "# Target\n",
    "y_full_train = np.array(dat_y_full_train).reshape(len(dat_y_full_train), 1)\n",
    "y_trunc1_train = np.array(dat_y_trunc1_train).reshape(len(dat_y_trunc1_train), 1)\n",
    "\n",
    "\n",
    "############## Test\n",
    "# Features\n",
    "x_full_test = np.array(dat_x_full_test).reshape(len(dat_x_full_test), np.array(dat_x_full_test).shape[2])\n",
    "x_trunc1_test = np.array(dat_x_trunc1_test).reshape(len(dat_x_trunc1_test), np.array(dat_x_trunc1_test).shape[2])\n",
    "\n",
    "# Target\n",
    "y_full_test = np.array(dat_y_full_test).reshape(len(dat_y_full_test), 1)\n",
    "y_trunc1_test = np.array(dat_y_trunc1_test).reshape(len(dat_y_trunc1_test), 1)\n",
    "\n",
    "print(x_full_train.shape)\n",
    "print(x_trunc1_train.shape)\n",
    "print(y_full_train.shape)\n",
    "print(y_trunc1_train.shape)\n",
    "\n",
    "print(x_full_test.shape)\n",
    "print(x_trunc1_test.shape)\n",
    "print(y_full_test.shape)\n",
    "print(y_trunc1_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a6afd1",
   "metadata": {},
   "source": [
    "### PCA and SVM Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cfa567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA reduction for training/test (100 components)\n",
    "x_full_train_tf, x_full_test_tf = train_test_transformed_PCA(x_full_train, x_full_test, 100)\n",
    "\n",
    "# SVM using 100 PCs\n",
    "svc = SVC(kernel = \"poly\")\n",
    "svc.fit(x_full_train_tf, y_full_train.ravel())\n",
    "predictions = svc.predict(x_full_test_tf)\n",
    "conf_matrix = confusion_matrix(predictions, y_full_test)\n",
    "print(conf_matrix)\n",
    "print(classification_report(y_full_test, \n",
    "                      predictions))\n",
    "print('Precision: %.3f' % precision_score(y_full_test, predictions))\n",
    "print('Recall: %.3f' % recall_score(y_full_test, predictions))\n",
    "print('F1: %.3f' % f1_score(y_full_test, predictions))\n",
    "print('Accuracy: %.3f' % accuracy_score(y_full_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b4d607",
   "metadata": {},
   "source": [
    "###  Truncated Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9d711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA reduction for training/test (100 components)\n",
    "x_trunc1_train_tf, x_trunc1_test_tf = train_test_transformed_PCA(x_trunc1_train, x_trunc1_test, 100)\n",
    "\n",
    "# SVM using 100 PCs\n",
    "svc = SVC(kernel = \"poly\")\n",
    "svc.fit(x_trunc1_train_tf, y_trunc1_train.ravel())\n",
    "predictions = svc.predict(x_trunc1_test_tf)\n",
    "conf_matrix = confusion_matrix(predictions, y_trunc1_test)\n",
    "print(conf_matrix)\n",
    "print(classification_report(y_trunc1_test, \n",
    "                      predictions))\n",
    "\n",
    "print('Precision: %.3f' % precision_score(y_trunc1_test, predictions))\n",
    "print('Recall: %.3f' % recall_score(y_trunc1_test, predictions))\n",
    "print('F1: %.3f' % f1_score(y_trunc1_test, predictions))\n",
    "print('Accuracy: %.3f' % accuracy_score(y_trunc1_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c0c05",
   "metadata": {},
   "source": [
    "### Just SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8df941",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Full\n",
    "svc = SVC(kernel = \"poly\")\n",
    "svc.fit(x_full_train, y_full_train.ravel())\n",
    "predictions = svc.predict(x_full_test)\n",
    "conf_matrix = confusion_matrix(predictions, y_full_test)\n",
    "print(conf_matrix)\n",
    "print(classification_report(y_full_test, \n",
    "                      predictions))\n",
    "\n",
    "print('Precision: %.3f' % precision_score(y_full_test, predictions))\n",
    "print('Recall: %.3f' % recall_score(y_full_test, predictions))\n",
    "print('F1: %.3f' % f1_score(y_full_test, predictions))\n",
    "print('Accuracy: %.3f' % accuracy_score(y_full_test, predictions))\n",
    "############ Truncated 1\n",
    "\n",
    "svc = SVC(kernel = \"poly\")\n",
    "svc.fit(x_trunc1_train, y_trunc1_train.ravel())\n",
    "predictions = svc.predict(x_trunc1_test)\n",
    "conf_matrix = confusion_matrix(predictions, y_trunc1_test)\n",
    "print(conf_matrix)\n",
    "print(classification_report(y_trunc1_test, \n",
    "                      predictions))\n",
    "\n",
    "print('Precision: %.3f' % precision_score(y_trunc1_test, predictions))\n",
    "print('Recall: %.3f' % recall_score(y_trunc1_test, predictions))\n",
    "print('F1: %.3f' % f1_score(y_trunc1_test, predictions))\n",
    "print('Accuracy: %.3f' % accuracy_score(y_trunc1_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
